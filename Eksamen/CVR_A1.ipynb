{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fd1def98-0dee-4feb-8a21-59cc0939c694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import json\n",
    "import time\n",
    "import tqdm\n",
    "import os, sys\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "547dfe14-af31-4bd5-9727-9abbcac5b66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /Users/frederikwintherbakholt/opt/anaconda3/lib/python3.9/site-packages (4.3.0)\n",
      "Requirement already satisfied: trio~=0.17 in /Users/frederikwintherbakholt/opt/anaconda3/lib/python3.9/site-packages (from selenium) (0.21.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /Users/frederikwintherbakholt/opt/anaconda3/lib/python3.9/site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in /Users/frederikwintherbakholt/opt/anaconda3/lib/python3.9/site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: async-generator>=1.9 in /Users/frederikwintherbakholt/opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: idna in /Users/frederikwintherbakholt/opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /Users/frederikwintherbakholt/opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: sniffio in /Users/frederikwintherbakholt/opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in /Users/frederikwintherbakholt/opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in /Users/frederikwintherbakholt/opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /Users/frederikwintherbakholt/opt/anaconda3/lib/python3.9/site-packages (from trio-websocket~=0.9->selenium) (1.1.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /Users/frederikwintherbakholt/opt/anaconda3/lib/python3.9/site-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in /Users/frederikwintherbakholt/opt/anaconda3/lib/python3.9/site-packages (from urllib3[secure,socks]~=1.26->selenium) (21.0.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in /Users/frederikwintherbakholt/opt/anaconda3/lib/python3.9/site-packages (from urllib3[secure,socks]~=1.26->selenium) (3.4.8)\n",
      "Requirement already satisfied: certifi in /Users/frederikwintherbakholt/opt/anaconda3/lib/python3.9/site-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/frederikwintherbakholt/opt/anaconda3/lib/python3.9/site-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /Users/frederikwintherbakholt/opt/anaconda3/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.20)\n",
      "Requirement already satisfied: six>=1.5.2 in /Users/frederikwintherbakholt/opt/anaconda3/lib/python3.9/site-packages (from pyOpenSSL>=0.14->urllib3[secure,socks]~=1.26->selenium) (1.16.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Users/frederikwintherbakholt/opt/anaconda3/lib/python3.9/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb9a5a6d-11fa-4adf-a278-4a38787f41b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver-manager in /Users/frederikwintherbakholt/opt/anaconda3/lib/python3.9/site-packages (3.8.3)\n",
      "Requirement already satisfied: requests in /Users/frederikwintherbakholt/opt/anaconda3/lib/python3.9/site-packages (from webdriver-manager) (2.26.0)\n",
      "Requirement already satisfied: python-dotenv in /Users/frederikwintherbakholt/opt/anaconda3/lib/python3.9/site-packages (from webdriver-manager) (0.20.0)\n",
      "Requirement already satisfied: tqdm in /Users/frederikwintherbakholt/opt/anaconda3/lib/python3.9/site-packages (from webdriver-manager) (4.62.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/frederikwintherbakholt/opt/anaconda3/lib/python3.9/site-packages (from requests->webdriver-manager) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/frederikwintherbakholt/opt/anaconda3/lib/python3.9/site-packages (from requests->webdriver-manager) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/frederikwintherbakholt/opt/anaconda3/lib/python3.9/site-packages (from requests->webdriver-manager) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/frederikwintherbakholt/opt/anaconda3/lib/python3.9/site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ed0c74a7-f936-4742-9acf-1b2b87bb622a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Funktioner \n",
    "\n",
    "\n",
    "# Define the log function to gather the log information\n",
    "def log(response,logfile,output_path=os.getcwd()):\n",
    "    # Open or create the csv file\n",
    "    if os.path.isfile(logfile): #If the log file exists, open it and allow for changes     \n",
    "        log = open(logfile,'a')\n",
    "    else: #If the log file does not exist, create it and make headers for the log variables\n",
    "        log = open(logfile,'w')\n",
    "        header = ['timestamp','status_code','length','output_file']\n",
    "        log.write(';'.join(header) + \"\\n\") #Make the headers and jump to new line\n",
    "        \n",
    "    # Gather log information\n",
    "    status_code = response.status_code #Status code from the request result\n",
    "    timestamp = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())) #Local time\n",
    "    length = len(response.text) #Length of the HTML-string\n",
    "    \n",
    "    # Open the log file and append the gathered log information\n",
    "    with open(logfile,'a') as log:\n",
    "        log.write(f'{timestamp};{status_code};{length};{output_path}' + \"\\n\") #Append the information and jump to new line\n",
    "        \n",
    "\n",
    "\n",
    "def open_cvr():\n",
    "    # Åbner CVR-hjemmesiden i virtuel-browser\n",
    "    url = 'https://datacvr.virk.dk/data/' #Link\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install()) #Open virtual browser\n",
    "    driver.implicitly_wait(100) #Setting waiting\n",
    "    driver.get(url)#Go to \n",
    "\n",
    "    #2\n",
    "\n",
    "    # Trykker accepter cookies \n",
    "    cookie = driver.find_element(By.XPATH, '//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"cpAcceptBtn\", \" \" ))]') #Here we use a CSS selector\n",
    "    cookie.click()\n",
    "\n",
    "    return driver  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "8ec4d6f1-e8a4-4f2b-8290-7d68c2ecba84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ss/pvm65dps049d1q9czm5j5qhm0000gn/T/ipykernel_15584/3892047240.py:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install()) #Open virtual browser\n"
     ]
    }
   ],
   "source": [
    "## Setup, kommer henter url, trykker cookies af, søger og trykker på virksomhed\n",
    "\n",
    "#1\n",
    "\n",
    "# Åbner CVR-hjemmesiden i virtuel-browser\n",
    "url = 'https://datacvr.virk.dk/data/' #Link\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install()) #Open virtual browser\n",
    "driver.implicitly_wait(100) #Setting waiting\n",
    "driver.get(url)#Go to \n",
    "\n",
    "#2\n",
    "\n",
    "# Trykker accepter cookies \n",
    "cookie = driver.find_element(By.XPATH, '//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"cpAcceptBtn\", \" \" ))]') #Here we use a CSS selector\n",
    "cookie.click()\n",
    "\n",
    "#3\n",
    "\n",
    "# Trykker på søgefelt\n",
    "seek = driver.find_element(By.XPATH, '//*[@id=\"main-content\"]/div[2]/div[1]/div/div/form/div/button[1]')\n",
    "seek.click()\n",
    "\n",
    "\n",
    "# Trykker på virksomheder\n",
    "seek = driver.find_element(By.XPATH, '//*[@id=\"main-content\"]/div[2]/div[2]/div/div/div/fieldset/ul/li[2]/label') #Here we use a CSS selector\n",
    "seek.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "02311c76-8221-4a26-aa11-e3943a1147e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LAVER LISTE MED ALLE DOBBELT KOMBINATION AF ALFABETET ###\n",
    "\n",
    "from string import ascii_lowercase\n",
    "import itertools\n",
    "\n",
    "def iter_all_strings():\n",
    "    for size in itertools.count(1):\n",
    "        for s in itertools.product(ascii_lowercase, repeat=size):\n",
    "            yield \"\".join(s)\n",
    "\n",
    "alphabeth = []            \n",
    "\n",
    "for s in iter_all_strings():\n",
    "    alphabeth.append(s)\n",
    "    if s == 'zz':\n",
    "        break\n",
    "        \n",
    "alphabeth = alphabeth[26:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "51b5cfd5-b395-49f8-a3bc-feaff1b9102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LAVER LISE MED URL'ER + ALFABET KOMBINATIONER\n",
    "\n",
    "def change_url(alphabeth):\n",
    "    url = f'https://datacvr.virk.dk/soegeresultater?fritekst={alphabeth}&sideIndex=0&enhedstype=virksomhed&virksomhedsform=10&virksomhedsstatus=aktiv&size=10'\n",
    "    return url\n",
    "\n",
    "url_list = []\n",
    "for x in alphabeth:\n",
    "    new_element = change_url(x)\n",
    "    url_list.append(new_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "46abfe93-abb9-494f-960e-ef835f4f64cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-08-11 16:13'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tid = datetime.datetime.now().strftime(\"%F %H:%M\")\n",
    "tid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "4b719a57-6a3f-49af-b78e-7f8b9c582882",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvr = []\n",
    "import datetime \n",
    "\n",
    "def maskine(url, bogstav):\n",
    "    driver.get(url)\n",
    "    with open(\"scraper_log.log\", \"a\") as f:\n",
    "        tid = datetime.datetime.now().strftime(\"%F %H:%M\")\n",
    "        besked = f\"Klokken er {tid}. Starter med at scrape...\"\n",
    "        print(besked, file=f); print(besked)\n",
    "        for i in range(300):\n",
    "            print(f\"Looper på side {i + 1} for bogstav {bogstav}\")\n",
    "            print(f\"Looper på side {i + 1} for bogstav {bogstav}\", file=f)\n",
    "            # Laver soup\n",
    "            driver.implicitly_wait(5)\n",
    "            time.sleep(1)\n",
    "            soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "\n",
    "            # Henter cvr på \n",
    "            firma = soup.find_all('div', class_ = \"value\")\n",
    "\n",
    "            cvrs = [int(t.text) for t in firma[0::3]]\n",
    "            for j in cvrs:\n",
    "                cvr.append(j)\n",
    "\n",
    "            current_url = driver.current_url\n",
    "            try:\n",
    "                next_ = driver.find_element(By.XPATH, '//*[@id=\"main-content\"]/div[2]/div[4]/div[2]/button[7]') #Here we use a CSS selector\n",
    "                next_.click()\n",
    "                time.sleep(0.2)\n",
    "                new_url = driver.current_url\n",
    "                if new_url == current_url:\n",
    "                    print(f\"Færdig med at scrape for bogstav {bogstav} (ikke flere sider)\")\n",
    "                    print(f\"Færdig med at scrape for bogstav {bogstav} (ikke flere sider)\", file=f)\n",
    "                    break\n",
    "\n",
    "            except:\n",
    "                print(f\"Færdig med at scrape for bogstav {bogstav} (fangede en error)\")\n",
    "                print(f\"Færdig med at scrape for bogstav {bogstav} (fangede en error)\", file=f)\n",
    "                break\n",
    "\n",
    "            if i% 5 == 0:\n",
    "                #Gemmer dataframen i en csv_fil\n",
    "                np.savetxt(f'cvr_scrape_data-{bogstav}.csv', cvr, fmt='% s')\n",
    "                continue #Continue to next iteration of the loop\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a1d1df-40cc-440b-ba25-e748ad9c97c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klokken er 2022-08-11 16:14. Starter med at scrape...\n",
      "Looper på side 1 for bogstav aa\n",
      "Looper på side 2 for bogstav aa\n",
      "Looper på side 3 for bogstav aa\n",
      "Looper på side 4 for bogstav aa\n",
      "Looper på side 5 for bogstav aa\n",
      "Looper på side 6 for bogstav aa\n",
      "Looper på side 7 for bogstav aa\n",
      "Looper på side 8 for bogstav aa\n",
      "Looper på side 9 for bogstav aa\n",
      "Looper på side 10 for bogstav aa\n",
      "Looper på side 11 for bogstav aa\n",
      "Looper på side 12 for bogstav aa\n",
      "Looper på side 13 for bogstav aa\n",
      "Looper på side 14 for bogstav aa\n",
      "Looper på side 15 for bogstav aa\n",
      "Looper på side 16 for bogstav aa\n",
      "Færdig med at scrape for bogstav aa (ikke flere sider)\n",
      "Klokken er 2022-08-11 16:14. Starter med at scrape...\n",
      "Looper på side 1 for bogstav ab\n",
      "Looper på side 2 for bogstav ab\n",
      "Looper på side 3 for bogstav ab\n",
      "Looper på side 4 for bogstav ab\n",
      "Looper på side 5 for bogstav ab\n",
      "Looper på side 6 for bogstav ab\n",
      "Looper på side 7 for bogstav ab\n",
      "Looper på side 8 for bogstav ab\n",
      "Looper på side 9 for bogstav ab\n",
      "Looper på side 10 for bogstav ab\n",
      "Looper på side 11 for bogstav ab\n",
      "Looper på side 12 for bogstav ab\n",
      "Looper på side 13 for bogstav ab\n",
      "Looper på side 14 for bogstav ab\n",
      "Looper på side 15 for bogstav ab\n",
      "Looper på side 16 for bogstav ab\n",
      "Looper på side 17 for bogstav ab\n",
      "Looper på side 18 for bogstav ab\n",
      "Looper på side 19 for bogstav ab\n",
      "Looper på side 20 for bogstav ab\n",
      "Looper på side 21 for bogstav ab\n",
      "Looper på side 22 for bogstav ab\n",
      "Færdig med at scrape for bogstav ab (ikke flere sider)\n",
      "Klokken er 2022-08-11 16:15. Starter med at scrape...\n",
      "Looper på side 1 for bogstav ac\n",
      "Looper på side 2 for bogstav ac\n",
      "Looper på side 3 for bogstav ac\n",
      "Looper på side 4 for bogstav ac\n",
      "Looper på side 5 for bogstav ac\n",
      "Looper på side 6 for bogstav ac\n",
      "Looper på side 7 for bogstav ac\n",
      "Looper på side 8 for bogstav ac\n",
      "Looper på side 9 for bogstav ac\n",
      "Looper på side 10 for bogstav ac\n",
      "Looper på side 11 for bogstav ac\n",
      "Looper på side 12 for bogstav ac\n",
      "Færdig med at scrape for bogstav ac (ikke flere sider)\n",
      "Klokken er 2022-08-11 16:15. Starter med at scrape...\n",
      "Looper på side 1 for bogstav ad\n",
      "Looper på side 2 for bogstav ad\n"
     ]
    }
   ],
   "source": [
    "for i, url in enumerate(url_list[:10]):\n",
    "    driver = maskine(url, alphabeth[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2c4b0e-78c1-46ca-930d-20fdcf95a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Del 1\n",
    "\n",
    "    \n",
    "for bogstav in url_list:\n",
    "    # Trykker på søgefelt\n",
    "    seek = driver.find_element(By.XPATH, '//*[@id=\"main-content\"]/div[2]/div[1]/div/div/form/div/button[1]')\n",
    "    seek.click()\n",
    "\n",
    "\n",
    "    # Trykker på virksomheder\n",
    "    seek = driver.find_element(By.XPATH, '//*[@id=\"main-content\"]/div[2]/div[2]/div/div/div/fieldset/ul/li[2]/label') #Here we use a CSS selector\n",
    "    seek.click()\n",
    "    \n",
    "    # Søg efter bosgtav \n",
    "    logfile = 'log.csv' \n",
    "    \n",
    "    \n",
    "    for i in range(300):\n",
    "        \n",
    "        # Laver soup\n",
    "        driver.implicitly_wait(5)\n",
    "        time.sleep(1)\n",
    "        soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "\n",
    "        # Henter cvr på \n",
    "        firma = soup.find_all('div', class_ = \"value\")\n",
    "\n",
    "        cvrs = [int(t.text) for t in firma[0::3]]\n",
    "        for j in cvrs:\n",
    "            cvr.append(j)\n",
    "\n",
    "        try:\n",
    "            next_ = driver.find_element(By.XPATH, '//*[@id=\"main-content\"]/div[2]/div[4]/div[2]/button[7]') #Here we use a CSS selector\n",
    "            next_.click()\n",
    "        except:\n",
    "            break\n",
    "        if i% 5 == 0:\n",
    "            #Gemmer dataframen i en csv_fil\n",
    "            np.savetxt(f'cvr_scrape_data-{bogstav}.csv', cvr, fmt='% s')\n",
    "            continue #Continue to next iteration of the loop\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3fc2bde2-eb9e-4bb3-a5b0-bce8ec6170f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('/Users/frederikwintherbakholt/Documents/GitHub/GruppeISDS22/Eksamen/cvr_scrape_data.csv')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>26692938</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30707176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28849842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36393254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33949928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16311677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2944</th>\n",
       "      <td>41248726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2945</th>\n",
       "      <td>38786652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2946</th>\n",
       "      <td>41853174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2947</th>\n",
       "      <td>41219165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2948</th>\n",
       "      <td>41234385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2949 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      26692938\n",
       "0     30707176\n",
       "1     28849842\n",
       "2     36393254\n",
       "3     33949928\n",
       "4     16311677\n",
       "...        ...\n",
       "2944  41248726\n",
       "2945  38786652\n",
       "2946  41853174\n",
       "2947  41219165\n",
       "2948  41234385\n",
       "\n",
       "[2949 rows x 1 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "filer = list(Path.cwd().glob(\"cvr_scrape_data*\"))\n",
    "print(filer)\n",
    "dfs = [pd.read_csv(file) for file in filer]\n",
    "pd.concat(dfs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "70201dd4-a24f-49a5-8cfc-1a4a675549ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Del 1 ORGINAL \n",
    "cvr = []\n",
    "logfile = 'log.csv'\n",
    "for i in range(300):\n",
    "    # Laver soup\n",
    "    driver.implicitly_wait(5)\n",
    "    time.sleep(1)\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "\n",
    "\n",
    "    # Henter cvr på \n",
    "    firma = soup.find_all('div', class_ = \"value\")\n",
    "\n",
    "    cvrs = [int(t.text) for t in firma[0::3]]\n",
    "    for j in cvrs:\n",
    "        cvr.append(j)\n",
    "        \n",
    "    try:\n",
    "        next_ = driver.find_element(By.XPATH, '//*[@id=\"main-content\"]/div[2]/div[4]/div[2]/button[7]') #Here we use a CSS selector\n",
    "        next_.click()\n",
    "    except:\n",
    "        break\n",
    "    if i% 5 == 0:\n",
    "        #Gemmer dataframen i en csv_fil\n",
    "        np.savetxt('cvr_test8.csv', cvr, fmt='% s')\n",
    "        continue #Continue to next iteration of the loop\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3ba87992-d1e8-4979-81da-57deb8d23678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12211"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv (r'test.csv')\n",
    "cvr_3 = np.unique(df)\n",
    "\n",
    "len(cvr_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
